{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Preface:**  \r\n",
    "The following Notebook is used to extract subsets of satellite data from NASA's GES DISC system. The original Notebook can be found at https://disc.gsfc.nasa.gov/information/howto?keywords=subset&title=How%20to%20Use%20the%20Web%20Services%20API%20for%20Subsetting. Downloading data from GES DISC requires a free Earthdata account, and to use this service through Python a .netrc authorization file must be created in the user directory (for details visit https://disc.gsfc.nasa.gov/data-access#python-requests). \r\n",
    "\r\n",
    "The main parameters used as input for this Notebook are:\r\n",
    "- Product Name (e.g. GPM_3IMERGHH_06)\r\n",
    "- Start Datetime\r\n",
    "- End Datetime\r\n",
    "- Minimum Latitude\r\n",
    "- Maximum Latitude\r\n",
    "- Minimum Longitude\r\n",
    "- Maximum Longitude\r\n",
    "\r\n",
    "Running this Notebook will download .nc4 files, which we read to obtain subset metadata and data, which can be printed out as a nested array at the end of the Notebook.\r\n",
    "\r\n",
    "----\r\n",
    "\r\n",
    "**Overview:**  \r\n",
    "The NASA Goddard Earth Sciences Data and Information Services Center (GES DISC) has developed an Application Program Interface (API) for interacting with our Web Processing Services in a programmatic way. The API is intended for users who would like to apply our subsetting services to numerous data granules spanning a long time range or a variety of data products -- circumstances that make using the Web browser interface quite inefficient.\r\n",
    "\r\n",
    "**Example:**  \r\n",
    "This example code demonstrates how to use the API to submit an asynchronous request to the GES DISC Subsetting Service using Python3. The API is a communication protocol that allows users to find the granules they need and download the desired data subsets. Information is passed back and forth in JavaScript Object Notation (JSON) format.\r\n",
    "\r\n",
    "**Prerequisites:**  \r\n",
    "This example code is written in Python3 and requires these libraries: sys, json, urllib3, certifi, requests, time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to import the required Python libraries. If any of the following import commands fail, check the local Python environment and install any missing packages. These lines will be necessary to run the rest of the cells:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "import json\r\n",
    "import urllib3\r\n",
    "import certifi\r\n",
    "import requests\r\n",
    "from time import sleep\r\n",
    "\r\n",
    "import netCDF4 as nc\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VIKTOR~1.KAP\\AppData\\Local\\Temp/ipykernel_185516/4054444185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetCDF4\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize the urllib PoolManager and set the base URL for the API requests that will be sent to the GES DISC subsetting service."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Create a urllib PoolManager instance to make requests.\r\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\r\n",
    "\r\n",
    "# Set the URL for the GES DISC subset service endpoint\r\n",
    "svcurl = 'https://disc.gsfc.nasa.gov/service/subset/jsonwsp'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a local general-purpose method that submits JSON-formatted WSP requests to the GES DISC server, checks for any errors, and then returns the service’s response. This method is created for convenience since this task will be repeated more than once. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# This method POSTs formatted JSON WSP requests to the GES DISC endpoint URL and returns the response\r\n",
    "def get_http_data(request):\r\n",
    "    hdrs = {'Content-Type': 'application/json',\r\n",
    "            'Accept'      : 'application/json'}\r\n",
    "    data = json.dumps(request) \r\n",
    "    r = http.request('POST', svcurl, body=data, headers=hdrs)\r\n",
    "    response = json.loads(r.data)   \r\n",
    "    # Check for errors\r\n",
    "    if response['type'] == 'jsonwsp/fault' :\r\n",
    "        print('API Error: faulty request')\r\n",
    "    return response"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data product used in this example is the Microwave Limb Sounder Level 2 Temperature Profile (ML2T_004). Three variables are selected: Temperature, TemperaturePrecision, and the Quality flag. The spatial domain is the global latitude band from 30oS to 30oN, vertical pressure levels range from 1000 to 100 hectoPascals (hPa), and the date range is 1-3 August 2015. The specifics of the subset are coded as local variables so they can be easily changed for different use cases. The desired spatial and temporal constraints, along with the dataset and variable specifications, are stored in a JSON-based Web Service Protocol (WSP) structure, which is named “subset_request”. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "# Define the parameters for the data subset\r\n",
    "product = 'GPM_3IMERGHH_06'\r\n",
    "begTime = '2021-01-01T08:00:00.000Z'\r\n",
    "endTime = '2021-01-01T10:00:00.000Z'\r\n",
    "minlon = -80.11\r\n",
    "maxlon = -79.67\r\n",
    "minlat = 43.04\r\n",
    "maxlat = 43.48\r\n",
    "varNames = ['/HDFEOS/SWATHS/Temperature/Data Fields/Temperature',\r\n",
    "            '/HDFEOS/SWATHS/Temperature/Data Fields/TemperaturePrecision',\r\n",
    "            '/HDFEOS/SWATHS/Temperature/Data Fields/Quality']\r\n",
    "\r\n",
    "# The dimension slice will be for pressure levels between 1000 and 100 hPa\r\n",
    "dimName = '/HDFEOS/SWATHS/Temperature/nLevels'\r\n",
    "dimVals = [1,2,3,4,5,6,7,8,9,10,11,12,13] \r\n",
    "dimSlice = []\r\n",
    "for i in range(len(dimVals)) :\r\n",
    "    dimSlice.append({'dimensionId': dimName, 'dimensionValue': dimVals[i]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters in this particular subset_request structure are: <code>methodname</code>, <code>type</code>, <code>version</code>, and <code>args</code>. The <code>args</code> contain additional parameters that control the specifics for the subset.  For this example, the args parameters are: <code>role</code>, <code>start</code>, <code>end</code>, <code>box</code>, <code>crop</code>, and <code>data</code>. The <code>start</code> and <code>end</code> parameters provide the desired time range. The <code>box</code> parameter specifies the desired spatial domain which will constrain the granule search -- only data granules that cover the domain will be returned. The <code>crop</code> parameter is a True/False flag indicating whether to perform spatial subsetting on the granules returned by the spatial search. Granules will not be trimmed to the specified spatial domain unless <code>crop</code> is set to True. The <code>data</code> parameter is another list containing attribute:value pairs that include the <code>datasetID</code>, the <code>variable</code> name, and the <code>slice</code> parameter, which contains a list of dimensionName:index pairs. Each desired variable must be listed separately within the <code>data</code> parameter. To retrieve all the variables in the data file, omit the variable:name pair. The <code>slice</code> parameter is also optional; leave it out to retrieve all the variable dimensions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "# Construct JSON WSP request for API method: subset\r\n",
    "subset_request = {\r\n",
    "    'methodname': 'subset',\r\n",
    "    'type': 'jsonwsp/request',\r\n",
    "    'version': '1.0',\r\n",
    "    'args': {\r\n",
    "        'role'  : 'subset',\r\n",
    "        'start' : begTime,\r\n",
    "        'end'   : endTime,\r\n",
    "        'box'   : [minlon, minlat, maxlon, maxlat],  \r\n",
    "        'crop'  : True,\r\n",
    "        'data'  : [{'datasetId': product}]\r\n",
    "    }\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a point+radius subset, use <code>lon</code>, <code>lat</code>, and <code>radius</code> parameters instead of <code>box</code>. For example, these values might be suitable for selecting radial subsets around Greenland:\n",
    "\n",
    "        'lon'    : -40.0,\n",
    "        'lat'    : 72.0,\n",
    "        'radius' : '12deg',\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next step, the JSON-formatted subset_request is POSTed to the GES DISC server. \n",
    "The Job ID is extracted from the response -- to be used later as a reference for the request."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# Submit the subset request to the GES DISC Server\r\n",
    "response = get_http_data(subset_request)\r\n",
    "\r\n",
    "# Report the JobID and initial status\r\n",
    "myJobId = response['result']['jobId']\r\n",
    "print('Job ID: '+myJobId)\r\n",
    "print('Job status: '+response['result']['Status'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Job ID: 615929c135b983905e275151\n",
      "Job status: Accepted\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point in the code, the job is running on the GES DISC server. The next step is to construct another JSON WSP request to periodically retrieve the job status, using the extracted Job ID. When the job is finished, check on the final status to ensure the job succeeded. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "# Construct JSON WSP request for API method: GetStatus\r\n",
    "status_request = {\r\n",
    "    'methodname': 'GetStatus',\r\n",
    "    'version': '1.0',\r\n",
    "    'type': 'jsonwsp/request',\r\n",
    "    'args': {'jobId': myJobId}\r\n",
    "}\r\n",
    "\r\n",
    "# Check on the job status after a brief nap\r\n",
    "while response['result']['Status'] in ['Accepted', 'Running']:\r\n",
    "    sleep(5)\r\n",
    "    response = get_http_data(status_request)\r\n",
    "    status  = response['result']['Status']\r\n",
    "    percent = response['result']['PercentCompleted']\r\n",
    "    print ('Job status: %s (%d%c complete)' % (status,percent,'%'))\r\n",
    "\r\n",
    "if response['result']['Status'] == 'Succeeded' :\r\n",
    "    print ('Job Finished:  %s' % response['result']['message'])\r\n",
    "else : \r\n",
    "    print('Job Failed: %s' % response['fault']['code'])\r\n",
    "    sys.exit(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Job status: Succeeded (100% complete)\n",
      "Job Finished:  Complete (GPM_3IMERGHH_06)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowing that the job has finished successfully, it is time to retrieve the results. The results of a subset request job are URLs: there are HTTP_Services URLs (one for every data granule in the time range of interest) plus links to any relevant documentation. Each HTTP_Services URL contains the specifics of the subset request encoded as facets. Data subsets and documentation files are downloaded using the requests Python library.\n",
    "\n",
    "There are two ways to retrieve the list of URLs when the subset job is finished:\n",
    "\n",
    "**Plan A:** \n",
    "Use the API method named GetResult. This method will return the URLs along with three additional attributes: a label, plus the beginning and ending time stamps for that particular data granule. The label serves as the filename for the downloaded subsets. \n",
    "\n",
    "**Plan B:**\n",
    "Retrieve a plain-text list of URLs in a single shot using the saved JobID. This is a shortcut to retrieve just the list of URLs without any of the other metadata. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the code for **Plan A**.\n",
    "The steps are to construct a third type of JSON WSP request that retrieves the results of this Job. When that request is submitted, the results are returned in multiple batches of 20 items, starting with item 0. The startIndex value in the results_request structure must be updated after each successive batch is retrieved."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# Construct JSON WSP request for API method: GetResult\r\n",
    "batchsize = 20\r\n",
    "results_request = {\r\n",
    "    'methodname': 'GetResult',\r\n",
    "    'version': '1.0',\r\n",
    "    'type': 'jsonwsp/request',\r\n",
    "    'args': {\r\n",
    "        'jobId': myJobId,\r\n",
    "        'count': batchsize,\r\n",
    "        'startIndex': 0\r\n",
    "    }\r\n",
    "}\r\n",
    "\r\n",
    "# Retrieve the results in JSON in multiple batches \r\n",
    "# Initialize variables, then submit the first GetResults request\r\n",
    "# Add the results from this batch to the list and increment the count\r\n",
    "results = []\r\n",
    "count = 0 \r\n",
    "response = get_http_data(results_request) \r\n",
    "count = count + response['result']['itemsPerPage']\r\n",
    "results.extend(response['result']['items']) \r\n",
    "\r\n",
    "# Increment the startIndex and keep asking for more results until we have them all\r\n",
    "total = response['result']['totalResults']\r\n",
    "while count < total :\r\n",
    "    results_request['args']['startIndex'] += batchsize \r\n",
    "    response = get_http_data(results_request) \r\n",
    "    count = count + response['result']['itemsPerPage']\r\n",
    "    results.extend(response['result']['items'])\r\n",
    "        \r\n",
    "# Check on the bookkeeping\r\n",
    "print('Retrieved %d out of %d expected items' % (len(results), total))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Retrieved 7 out of 7 expected items\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below is the code for **Plan B**. \n",
    "Construct a request using the saved JobID and retrieve the results with the requests library. If the requests.get() method does not return an error, the URLs are stored locally and printed out for informational purposes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Retrieve a plain-text list of results in a single shot using the saved JobID\r\n",
    "result = requests.get('https://disc.gsfc.nasa.gov/api/jobs/results/'+myJobId)\r\n",
    "try:\r\n",
    "    result.raise_for_status()\r\n",
    "    urls = result.text.split('\\n')\r\n",
    "    for i in urls : print('\\n%s' % i)\r\n",
    "except :\r\n",
    "    print('Request returned error code %d' % result.status_code)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "https://docserver.gesdisc.eosdis.nasa.gov/public/project/GPM/IMERG_ATBD_V06.pdf\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/doc/README.GPM.pdf\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S080000-E082959.0480.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S083000-E085959.0510.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S090000-E092959.0540.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S093000-E095959.0570.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S100000-E102959.0600.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is important to remember that the results returned at this point are not data files, but lists of URLs. Most of the URLs will contain HTTP_services requests to actually do the subsetting and return the data, but some of them may be links to documentation files pertaining to the dataset in question. It is worthwhile to separate the document URLs from the HTTP_services URLs in case the documentation has already been retrieved. The way we do this is to check for start and end attributes, which are always associated with HTTP_services URLs. \n",
    "\n",
    "The remainder of the example code assumes the use of **Plan A** because it makes use of this extra metadata. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# Sort the results into documents and URLs\r\n",
    "docs = []\r\n",
    "urls = []\r\n",
    "for item in results :\r\n",
    "    try:\r\n",
    "        if item['start'] and item['end'] : urls.append(item) \r\n",
    "    except:\r\n",
    "        docs.append(item)\r\n",
    "\r\n",
    "# Print out the documentation links, but do not download them\r\n",
    "print('\\nDocumentation:')\r\n",
    "for item in docs : print(item['label']+': '+item['link'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Documentation:\n",
      "IMERG_ATBD_V06.pdf: https://docserver.gesdisc.eosdis.nasa.gov/public/project/GPM/IMERG_ATBD_V06.pdf\n",
      "README Document: https://gpm1.gesdisc.eosdis.nasa.gov/data/GPM_L3/doc/README.GPM.pdf\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final step is to use the requests.get() method to invoke each HTTP_Services URL and download the data files. The contents of the label attribute are used here as the output file name, but the name can be any string. It is important to download each file one at a time, in series rather than in parallel, in order to avoid overloading the GES DISC servers. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def netReader(netFile, variableName):\r\n",
    "    '''Added to read .nc4 files.'''\r\n",
    "    data = nc.Dataset(netFile)\r\n",
    "\r\n",
    "    array = np.ma.getdata(data.variables['precipitationCal'][0,:,:])\r\n",
    "    return array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Use the requests library to submit the HTTP_Services URLs and write out the results.\r\n",
    "print('\\nHTTP_services output:')\r\n",
    "array = []\r\n",
    "for item in urls :\r\n",
    "    URL = item['link'] \r\n",
    "    print(URL)\r\n",
    "    result = requests.get(URL)\r\n",
    "    try:\r\n",
    "        result.raise_for_status()\r\n",
    "        outfn = item['label']\r\n",
    "        f = open(outfn,'wb')\r\n",
    "        f.write(result.content)\r\n",
    "        f.close()\r\n",
    "        print(outfn)\r\n",
    "        array = np.append([netReader(outfn, 'precipitationCal')],axis=0)\r\n",
    "    except:\r\n",
    "        print('Error! Status code is %d for this URL:\\n%s' % (result.status.code,URL))\r\n",
    "        print('Help for downloading data is at https://disc.gsfc.nasa.gov/data-access')\r\n",
    "\r\n",
    "print(array)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "HTTP_services output:\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S080000-E082959.0480.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "3B-HHR.MS.MRG.3IMERG.20210101-S080000-E082959.0480.V06B.HDF5.nc4\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S083000-E085959.0510.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "3B-HHR.MS.MRG.3IMERG.20210101-S083000-E085959.0510.V06B.HDF5.nc4\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S090000-E092959.0540.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "3B-HHR.MS.MRG.3IMERG.20210101-S090000-E092959.0540.V06B.HDF5.nc4\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S093000-E095959.0570.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "3B-HHR.MS.MRG.3IMERG.20210101-S093000-E095959.0570.V06B.HDF5.nc4\n",
      "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/GPM_L3/GPM_3IMERGHH.06/2021/001/3B-HHR.MS.MRG.3IMERG.20210101-S100000-E102959.0600.V06B.HDF5.nc4?precipitationQualityIndex[0:0][998:1003][1330:1334],IRkalmanFilterWeight[0:0][998:1003][1330:1334],HQprecipSource[0:0][998:1003][1330:1334],precipitationCal[0:0][998:1003][1330:1334],lat_bnds[1330:1334][0:1],precipitationUncal[0:0][998:1003][1330:1334],HQprecipitation[0:0][998:1003][1330:1334],probabilityLiquidPrecipitation[0:0][998:1003][1330:1334],HQobservationTime[0:0][998:1003][1330:1334],randomError[0:0][998:1003][1330:1334],time_bnds[0:0][0:1],IRprecipitation[0:0][998:1003][1330:1334],lon_bnds[998:1003][0:1],time,lon[998:1003],lat[1330:1334],latv,nv,lonv\n",
      "3B-HHR.MS.MRG.3IMERG.20210101-S100000-E102959.0600.V06B.HDF5.nc4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the code above does not succeed in your particular environment, please check the [Earthdata wiki page](https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+Python) for alternative Python examples. [The GES DISC guide to data access](https://disc.gsfc.nasa.gov/data-access) has some additional options for downloading data URLs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Additional Info:**  \n",
    "[Complete reference documentation for the GES DISC Subsetting Service API](https://disc.gsfc.nasa.gov/service/subset)  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font size=\"1\">THE SUBJECT FILE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT FILE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT FILE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT FILE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT FILE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE SUBJECT FILE, AND DISTRIBUTES IT \"AS IS.\"</font>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "d407ca0824dc5e9a1aad3245a17ec43d15ea01dbf7038a0621de2747de6d46f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}